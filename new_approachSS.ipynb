{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc523d92",
   "metadata": {},
   "source": [
    "Front Matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eea6ccb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796cc78e",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5196f56",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m file_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the CSV file\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Convert date column to datetime\u001b[39;00m\n\u001b[0;32m      7\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\yuvbo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yuvbo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\yuvbo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yuvbo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\yuvbo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.csv'"
     ]
    }
   ],
   "source": [
    "file_name=\"data.csv\"\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Convert date column to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Convert columns to numeric\n",
    "for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "    df[col] = pd.to_numeric(df[col])\n",
    "\n",
    "# Replace NaN values with column mean\n",
    "df.fillna(df.select_dtypes(include=['number']).mean(), inplace=True)\n",
    "\n",
    "# Calculate technical indicators\n",
    "# Moving Averages\n",
    "ma_day = [10, 50, 100]\n",
    "for ma in ma_day:\n",
    "    column_name = f\"MA for {ma} days\"\n",
    "    df[column_name] = df['close'].rolling(window=ma).mean()\n",
    "\n",
    "# Daily Return\n",
    "df['Daily Return'] = df['close'].pct_change()\n",
    "\n",
    "# EMAs for various periods\n",
    "df['ema7'] = df['close'].ewm(span=7, adjust=False).mean()\n",
    "df['ema14'] = df['close'].ewm(span=14, adjust=False).mean()\n",
    "df['ema30'] = df['close'].ewm(span=30, adjust=False).mean()\n",
    "df['ema60'] = df['close'].ewm(span=60, adjust=False).mean()\n",
    "\n",
    "# Sort by date and reset index\n",
    "df = df.sort_values('date')\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Fill any remaining NaN values\n",
    "df.fillna(method='bfill', inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4e2a02",
   "metadata": {},
   "source": [
    "BiLSTM Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7027db48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "def split_data(df, test_size=0.2):\n",
    "    train_size = int(len(df) * (1 - test_size))\n",
    "    train_data = df[:train_size]\n",
    "    test_data = df[train_size:]\n",
    "    return train_data, test_data\n",
    "\n",
    "# Prepare data for LSTM model\n",
    "def prepare_data_for_lstm(df, target_col='close', lookback=60):\n",
    "    \"\"\"Prepare data for LSTM model with a lookback period\"\"\"\n",
    "    # Scale the data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = scaler.fit_transform(df[target_col].values.reshape(-1, 1))\n",
    "    \n",
    "    X, y = [], []\n",
    "    for i in range(lookback, len(scaled_data)):\n",
    "        X.append(scaled_data[i-lookback:i, 0])\n",
    "        y.append(scaled_data[i, 0])\n",
    "    \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "    \n",
    "    return X, y, scaler\n",
    "\n",
    "# Create Bidirectional LSTM model for improved performance\n",
    "def create_bilstm_model(input_shape):\n",
    "    \"\"\"Create Bidirectional LSTM model for stock price prediction\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units=50, return_sequences=True, input_shape=input_shape)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Bidirectional(LSTM(units=50, return_sequences=False)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=1))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the model\n",
    "def train_lstm_model(df, lookback=60, epochs=12, batch_size=32, test_size=0.2):\n",
    "    \"\"\"Train and evaluate LSTM model\"\"\"\n",
    "    # Split data\n",
    "    train_data, test_data = split_data(df, test_size)\n",
    "    \n",
    "    # Prepare data for LSTM\n",
    "    X_train, y_train, scaler = prepare_data_for_lstm(train_data, lookback=lookback)\n",
    "    X_test, y_test, _ = prepare_data_for_lstm(pd.concat([train_data.iloc[-lookback:], test_data]), lookback=lookback)\n",
    "    \n",
    "    # Create and train model\n",
    "    model = create_bilstm_model((X_train.shape[1], 1))\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, \n",
    "                        validation_data=(X_test, y_test), verbose=1)\n",
    "    \n",
    "    # Predictions\n",
    "    train_predictions = model.predict(X_train)\n",
    "    test_predictions = model.predict(X_test)\n",
    "    \n",
    "    # Inverse scaling\n",
    "    train_predictions = scaler.inverse_transform(train_predictions)\n",
    "    y_train_actual = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "    test_predictions = scaler.inverse_transform(test_predictions)\n",
    "    y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "    return model, scaler, test_predictions, y_test_actual, test_data['date'].reset_index(drop=True), history\n",
    "\n",
    "# EMA constraint function\n",
    "def apply_ema_constraint(prediction, df, window=60, max_deviation=0.1):\n",
    "    \"\"\"Apply EMA constraint to ensure prediction is within reasonable range\"\"\"\n",
    "    # Calculate EMA of actual prices\n",
    "    ema = df['close'].ewm(span=window, adjust=False).mean().iloc[-1]\n",
    "    \n",
    "    # Set upper and lower bounds based on EMA\n",
    "    upper_bound = ema * (1 + max_deviation)\n",
    "    lower_bound = ema * (1 - max_deviation)\n",
    "    \n",
    "    # Apply constraints\n",
    "    if prediction > upper_bound:\n",
    "        constrained_prediction = upper_bound\n",
    "    elif prediction < lower_bound:\n",
    "        constrained_prediction = lower_bound\n",
    "    else:\n",
    "        constrained_prediction = prediction\n",
    "    \n",
    "    return constrained_prediction, ema, lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ba3e66",
   "metadata": {},
   "source": [
    "Training and Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be34c586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "lookback = 60\n",
    "if len(df) <= lookback:\n",
    "    print(f\"Not enough data for stock. Skipping...\")\n",
    "\n",
    "# Train model\n",
    "model, scaler, test_predictions, y_test_actual, test_dates, history = train_lstm_model(\n",
    "    df, lookback=lookback, epochs=12, batch_size=32\n",
    ")\n",
    "\n",
    "# Plot actual vs predicted prices for test set\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(test_dates, y_test_actual, label='Actual Prices')\n",
    "plt.plot(test_dates, test_predictions, label='Predicted Prices')\n",
    "plt.title(f'Stock Price Prediction for {stock}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146b3a55",
   "metadata": {},
   "source": [
    "Next Day Price Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e97feda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict next day's price\n",
    "def predict_next_day(model, df, scaler, lookback=60, ema_window=60, max_deviation=0.1):\n",
    "    \"\"\"Predict next day's stock price and apply EMA constraint\"\"\"\n",
    "    # Get the last 'lookback' days of data\n",
    "    last_sequence = df['close'].values[-lookback:]\n",
    "    \n",
    "    # Scale the data\n",
    "    last_sequence = scaler.transform(last_sequence.reshape(-1, 1))\n",
    "    \n",
    "    # Reshape for LSTM input\n",
    "    last_sequence = np.reshape(last_sequence, (1, lookback, 1))\n",
    "    \n",
    "    # Predict\n",
    "    next_day_scaled = model.predict(last_sequence)\n",
    "    \n",
    "    # Inverse transform\n",
    "    next_day_price = scaler.inverse_transform(next_day_scaled)[0, 0]\n",
    "    \n",
    "    # Apply EMA constraint\n",
    "    constrained_price, ema, lower_bound, upper_bound = apply_ema_constraint(\n",
    "        next_day_price, df, window=ema_window, max_deviation=max_deviation\n",
    "    )\n",
    "    \n",
    "    return constrained_price, next_day_price, ema, lower_bound, upper_bound\n",
    "\n",
    "\n",
    "        \n",
    "# Predict next day price\n",
    "constrained_price, raw_prediction, ema, lower_bound, upper_bound = predict_next_day(\n",
    "    model, df, scaler, lookback=60, ema_window=60, max_deviation=0.1\n",
    ")\n",
    "prediction={}\n",
    "# Store prediction\n",
    "prediction['stock'] = {\n",
    "    \"current_price\": df['close'].iloc[-1],\n",
    "    \"raw_prediction\": raw_prediction,\n",
    "    \"constrained_prediction\": constrained_price,\n",
    "    \"ema_60\": ema,\n",
    "    \"lower_bound\": lower_bound,\n",
    "    \"upper_bound\": upper_bound\n",
    "}\n",
    "\n",
    "# Print prediction\n",
    "print(f\"IBM Stock:\")\n",
    "print(f\"  Current price: ${df['close'].iloc[-1]:.2f}\")\n",
    "print(f\"  Raw prediction: ${raw_prediction:.2f}\")\n",
    "print(f\"  EMA (60 days): ${ema:.2f}\")\n",
    "print(f\"  Allowed range: ${lower_bound:.2f} to ${upper_bound:.2f}\")\n",
    "print(f\"  Final prediction: ${constrained_price:.2f}\")\n",
    "print(f\"  Expected change: ${constrained_price - df['close'].iloc[-1]:.2f} ({(constrained_price - df['close'].iloc[-1]) / df['close'].iloc[-1] * 100:.2f}%)\")\n",
    "print()\n",
    "\n",
    "# Create a summary DataFrame of all predictions\n",
    "prediction_summary = pd.DataFrame(columns=[\n",
    "    'Sector', 'Stock', 'Current Price', 'Predicted Price', 'Expected Change', 'Expected Change (%)'\n",
    "])\n",
    "\n",
    "row = 0\n",
    "for sector, stocks in prediction.items():\n",
    "    for stock, pred in stocks.items():\n",
    "        prediction_summary.loc[row] = [\n",
    "            sector,\n",
    "            stock,\n",
    "            pred['current_price'],\n",
    "            pred['constrained_prediction'],\n",
    "            pred['constrained_prediction'] - pred['current_price'],\n",
    "            (pred['constrained_prediction'] - pred['current_price']) / pred['current_price'] * 100\n",
    "        ]\n",
    "        row += 1\n",
    "\n",
    "# Sort by expected change percentage\n",
    "prediction_summary = prediction_summary.sort_values('Expected Change (%)', ascending=False)\n",
    "prediction_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daae9126",
   "metadata": {},
   "source": [
    "NOVELTY: Bayesian Networks to evaluate External Market Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02868c3",
   "metadata": {},
   "source": [
    "Front Matter 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11954ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from fredapi import Fred\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2928c6fd",
   "metadata": {},
   "source": [
    "Data Collection and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35404652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FRED API (use your API key)\n",
    "fred = Fred(api_key='YOUR_FRED_API_KEY')\n",
    "\n",
    "# Define date range\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2023-12-31'\n",
    "\n",
    "# IBM stock data\n",
    "ibm = yf.download('IBM', start=start_date, end=end_date)  # IBM stock price & volume\n",
    "print(\"IBM Columns:\", ibm.columns)\n",
    "\n",
    "# IBM Option Chain for Implied Volatility\n",
    "ticker = yf.Ticker('IBM')\n",
    "options_dates = ticker.options\n",
    "option_chain = ticker.option_chain(options_dates[0])  # Take nearest expiry\n",
    "ibm_iv = option_chain.calls['impliedVolatility'].mean()  # Average IV from calls\n",
    "\n",
    "# Market index (S&P 500)\n",
    "sp500 = yf.download('^GSPC', start=start_date, end=end_date)  # Market Index Return\n",
    "\n",
    "# Sector ETF (Technology ETF XLK)\n",
    "xlk = yf.download('XLK', start=start_date, end=end_date)  # Sector Performance Index\n",
    "\n",
    "# VIX index (daily volatility measure)\n",
    "vix = yf.download('^VIX', start=start_date, end=end_date)  # VIX (CBOE Volatility Index)\n",
    "\n",
    "# Credit Spread proxy using BAA - AAA spread from FRED\n",
    "baa_yield = fred.get_series('BAA10Y', observation_start=start_date, observation_end=end_date)  # BAA corporate bonds\n",
    "aaa_yield = fred.get_series('AAA10Y', observation_start=start_date, observation_end=end_date)  # AAA corporate bonds\n",
    "credit_spread = (baa_yield - aaa_yield).resample('D').ffill()  # Credit Spreads\n",
    "\n",
    "# 10-Year Treasury Yield from FRED\n",
    "treasury_yield = fred.get_series('GS10', observation_start=start_date, observation_end=end_date)  # Risk-free rate\n",
    "treasury_yield = treasury_yield.resample('D').ffill()\n",
    "\n",
    "# CPI data from FRED\n",
    "cpi = fred.get_series('CPIAUCSL', observation_start=start_date, observation_end=end_date)  # Inflation proxy\n",
    "cpi = cpi.resample('D').ffill()\n",
    "\n",
    "# Align all data to the same date index\n",
    "combined_data = pd.DataFrame(index=ibm.index)\n",
    "combined_data['IBM_Close'] = ibm['Close']  # IBM Close\n",
    "combined_data['IBM_Volume'] = ibm['Volume']  # Trading Volume\n",
    "combined_data['SP500_Close'] = sp500['Close']  # Market Index Return\n",
    "combined_data['XLK_Close'] = xlk['Close']  # Sector Performance Index\n",
    "combined_data['VIX'] = vix['Close']  # VIX\n",
    "combined_data['Treasury_Yield'] = treasury_yield.reindex(ibm.index)  # Treasury Yield\n",
    "combined_data['CPI'] = cpi.reindex(ibm.index)  # CPI\n",
    "combined_data['Credit_Spread'] = credit_spread.reindex(ibm.index)  # Credit Spread\n",
    "combined_data['Implied_Volatility'] = ibm_iv  # Static IV for now\n",
    "\n",
    "# Drop rows with missing values\n",
    "combined_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50413b02",
   "metadata": {},
   "source": [
    "Correlation Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587bbd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IBM daily returns\n",
    "combined_data['IBM_Returns'] = combined_data['IBM_Close'].pct_change()\n",
    "\n",
    "# Calculate company-specific features directly from combined_data\n",
    "combined_data['IBM_Volatility'] = combined_data['IBM_Close'].pct_change().rolling(window=30).std()  # Historical Volatility\n",
    "combined_data['IBM_Momentum'] = combined_data['IBM_Close'] / combined_data['IBM_Close'].shift(20) - 1  # Price Momentum (20-day)\n",
    "\n",
    "# Calculate daily returns for other relevant factors\n",
    "combined_data['SP500_Returns'] = combined_data['SP500_Close'].pct_change()\n",
    "combined_data['XLK_Returns'] = combined_data['XLK_Close'].pct_change()\n",
    "combined_data['VIX_Change'] = combined_data['VIX'].pct_change()\n",
    "combined_data['Treasury_Yield_Change'] = combined_data['Treasury_Yield'].pct_change()\n",
    "combined_data['CPI_Change'] = combined_data['CPI'].pct_change()\n",
    "combined_data['Credit_Spread_Change'] = combined_data['Credit_Spread'].pct_change()\n",
    "combined_data['Volume_Change'] = combined_data['IBM_Volume'].pct_change()\n",
    "\n",
    "# Drop NA after pct_change\n",
    "combined_data.dropna(inplace=True)\n",
    "\n",
    "# Calculate threshold values for each factor based on percentiles\n",
    "thresholds = {}\n",
    "for factor in [\n",
    "    'SP500_Returns', 'XLK_Returns', 'VIX_Change', 'Treasury_Yield_Change',\n",
    "    'CPI_Change', 'Credit_Spread_Change', 'IBM_Volatility', 'IBM_Momentum',\n",
    "    'Volume_Change', 'Implied_Volatility'\n",
    "]:\n",
    "    if factor == 'Implied_Volatility':\n",
    "        thresholds[factor] = [combined_data[factor].mean() * 0.85, combined_data[factor].mean() * 1.15]\n",
    "    else:\n",
    "        thresholds[factor] = [\n",
    "            combined_data[factor].quantile(0.33),\n",
    "            combined_data[factor].quantile(0.66)\n",
    "        ]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = combined_data[[\n",
    "    'IBM_Returns', 'SP500_Returns', 'XLK_Returns', 'VIX_Change', \n",
    "    'Treasury_Yield_Change', 'CPI_Change', 'Credit_Spread_Change',\n",
    "    'IBM_Volatility', 'IBM_Momentum', 'Volume_Change', 'Implied_Volatility'\n",
    "]].corr()\n",
    "\n",
    "# Show correlations specifically to IBM Returns\n",
    "ibm_corr = correlation_matrix['IBM_Returns'].drop('IBM_Returns')\n",
    "\n",
    "print(\"Correlation of external factors with IBM Returns:\")\n",
    "print(ibm_corr)\n",
    "\n",
    "# Optional: Plot heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91129fad",
   "metadata": {},
   "source": [
    "Construct Conditional Probability Table for External Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbaa7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cpt_value(value, thresholds, correlation):\n",
    "    shift = abs(correlation) * 0.25\n",
    "    base = 1 / 3\n",
    "    if value < thresholds[0]:\n",
    "        return base - shift if correlation > 0 else base + shift\n",
    "    elif value > thresholds[1]:\n",
    "        return base + shift if correlation > 0 else base - shift\n",
    "    else:\n",
    "        return base\n",
    "\n",
    "# Use most recent values for CPT evaluation\n",
    "latest = combined_data.iloc[-1]\n",
    "\n",
    "# Compute conditional probabilities\n",
    "cp_values = {}\n",
    "for factor, corr in ibm_corr.items():\n",
    "    val = latest[factor]\n",
    "    t = thresholds[factor]\n",
    "    cp = get_cpt_value(val, t, corr)\n",
    "    cp_values[factor] = cp\n",
    "    print(f\"{factor}: value={val:.4f}, correlation={corr:.3f}, CP={cp:.3f}\")\n",
    "\n",
    "# Compute overall conditional probability (product of independent CPs)\n",
    "risk_factor = 1\n",
    "for cp in cp_values.values():\n",
    "    risk_factor *= cp\n",
    "\n",
    "print(\"\\nFinal Conditional Risk Factor for IBM:\", risk_factor)\n",
    "\n",
    "# Example\n",
    "predicted_price = prediction['stock']['constrained_prediction']\n",
    "adjusted_price = predicted_price * risk_factor\n",
    "print(\"Predicted Price:\", predicted_price)\n",
    "print(\"Adjusted Price after Risk Assessment:\", adjusted_price)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

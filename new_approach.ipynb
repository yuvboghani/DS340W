{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7ef1c07",
   "metadata": {},
   "source": [
    "Front Matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc633086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226d9d22",
   "metadata": {},
   "source": [
    "Sector Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f513a77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sectors and their top 10 performing stocks\n",
    "sectors = {\n",
    "    \"technology\": [\"AAPL\", \"MSFT\", \"GOOGL\", \"NVDA\", \"META\", \"ORCL\", \"ADBE\", \"CRM\", \"INTC\", \"AMD\", \"IBM\"],\n",
    "    \"medicine\": [\"PFE\", \"JNJ\", \"MRK\", \"ABBV\", \"BMY\", \"LLY\", \"AMGN\", \"GILD\", \"REGN\", \"BIIB\"],\n",
    "    \"industrial\": [\"GE\", \"CAT\", \"HON\", \"BA\", \"MMM\", \"DE\", \"LMT\", \"RTX\", \"NOC\", \"EMR\"],\n",
    "    \"finance\": [\"JPM\", \"BAC\", \"WFC\", \"C\", \"GS\", \"MS\", \"AXP\", \"BK\", \"USB\", \"PNC\"],\n",
    "    \"consumer_discretionary\": [\"TSLA\", \"AMZN\", \"HD\", \"LOW\", \"NKE\", \"SBUX\", \"MCD\", \"TGT\", \"DIS\", \"ROST\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97d6119",
   "metadata": {},
   "source": [
    "Data Retrival and CSV Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a539e35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving data for AAPL...\n",
      "Data saved to technology_AAPL.csv\n",
      "Retrieving data for MSFT...\n",
      "Data saved to technology_MSFT.csv\n",
      "Retrieving data for GOOGL...\n",
      "Data saved to technology_GOOGL.csv\n",
      "Retrieving data for NVDA...\n",
      "Data saved to technology_NVDA.csv\n",
      "Retrieving data for META...\n",
      "Data saved to technology_META.csv\n",
      "Retrieving data for ORCL...\n",
      "Data saved to technology_ORCL.csv\n",
      "Retrieving data for ADBE...\n",
      "Data saved to technology_ADBE.csv\n",
      "Retrieving data for CRM...\n",
      "Data saved to technology_CRM.csv\n",
      "Retrieving data for INTC...\n",
      "Data saved to technology_INTC.csv\n",
      "Retrieving data for AMD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [09:43<38:55, 583.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to technology_AMD.csv\n",
      "Retrieving data for PFE...\n",
      "Data saved to medicine_PFE.csv\n",
      "Retrieving data for JNJ...\n",
      "Data saved to medicine_JNJ.csv\n",
      "Retrieving data for MRK...\n",
      "Data saved to medicine_MRK.csv\n",
      "Retrieving data for ABBV...\n",
      "Data saved to medicine_ABBV.csv\n",
      "Retrieving data for BMY...\n",
      "Data saved to medicine_BMY.csv\n",
      "Retrieving data for LLY...\n"
     ]
    }
   ],
   "source": [
    "# Alpha Vantage API key\n",
    "key = open('AV_API_Key.txt').read().strip()\n",
    "\n",
    "# Function to request stock price data and save to CSV\n",
    "def request_and_save_stock_data(symbol, sector, token):\n",
    "    q_string = 'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={}&outputsize=full&apikey={}'\n",
    "    \n",
    "    # Request data from Alpha Vantage\n",
    "    print(f\"Retrieving data for {symbol}...\")\n",
    "    r = requests.get(q_string.format(symbol, token))\n",
    "    if \"Time Series (Daily)\" not in r.json():\n",
    "        print(f\"Error retrieving data for {symbol}\")\n",
    "        return None\n",
    "\n",
    "    # Parse data into a DataFrame\n",
    "    date = []\n",
    "    df = pd.DataFrame()\n",
    "    for i in r.json()['Time Series (Daily)'].keys():\n",
    "        date.append(i)\n",
    "        row = pd.DataFrame.from_dict(r.json()['Time Series (Daily)'][i], orient='index').reset_index().T[1:]\n",
    "        df = pd.concat([df, row], ignore_index=True)\n",
    "    df.columns = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "    df['date'] = date\n",
    "\n",
    "    # Save to CSV\n",
    "    file_name = f\"{sector}_{symbol}.csv\"\n",
    "    df.to_csv(file_name, index=False)\n",
    "    print(f\"Data saved to {file_name}\")\n",
    "    return file_name\n",
    "\n",
    "# Iterate over sectors and stocks to save data\n",
    "stock_csv_mapping = {}\n",
    "for sector, stocks in tqdm(sectors.items()):\n",
    "    stock_csv_mapping[sector] = {}\n",
    "    for stock in stocks:\n",
    "        file_name = request_and_save_stock_data(stock, sector, key)\n",
    "        if file_name:\n",
    "            stock_csv_mapping[sector][stock] = file_name\n",
    "\n",
    "# Output the mapping of stocks to their CSV files\n",
    "stock_csv_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1215db01",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30920ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform basic data preprocessing on all CSV files\n",
    "preprocessed_data = {}\n",
    "\n",
    "for sector, stocks in stock_csv_mapping.items():\n",
    "    preprocessed_data[sector] = {}\n",
    "    for stock, file_name in stocks.items():\n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(file_name)\n",
    "        \n",
    "        # Convert date column to datetime\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        \n",
    "        # Convert columns to numeric\n",
    "        for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "            df[col] = pd.to_numeric(df[col])\n",
    "        \n",
    "        # Replace NaN values with column mean\n",
    "        df.fillna(df.select_dtypes(include=['number']).mean(), inplace=True)\n",
    "        \n",
    "        # Calculate technical indicators\n",
    "        # Moving Averages\n",
    "        ma_day = [10, 50, 100]\n",
    "        for ma in ma_day:\n",
    "            column_name = f\"MA for {ma} days\"\n",
    "            df[column_name] = df['close'].rolling(window=ma).mean()\n",
    "        \n",
    "        # Daily Return\n",
    "        df['Daily Return'] = df['close'].pct_change()\n",
    "        \n",
    "        # EMAs for various periods\n",
    "        df['ema7'] = df['close'].ewm(span=7, adjust=False).mean()\n",
    "        df['ema14'] = df['close'].ewm(span=14, adjust=False).mean()\n",
    "        df['ema30'] = df['close'].ewm(span=30, adjust=False).mean()\n",
    "        df['ema60'] = df['close'].ewm(span=60, adjust=False).mean()\n",
    "\n",
    "        # Sort by date and reset index\n",
    "        df = df.sort_values('date')\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        # Fill any remaining NaN values\n",
    "        df.fillna(method='bfill', inplace=True)\n",
    "        \n",
    "        # Save the preprocessed DataFrame back to the same CSV file\n",
    "        df.to_csv(file_name, index=False)\n",
    "        \n",
    "        # Store the preprocessed DataFrame in the dictionary\n",
    "        preprocessed_data[sector][stock] = df\n",
    "\n",
    "# Output the first few rows of a sample preprocessed stock\n",
    "sample_sector = list(preprocessed_data.keys())[0]\n",
    "sample_stock = list(preprocessed_data[sample_sector].keys())[0]\n",
    "preprocessed_data[sample_sector][sample_stock].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a50604",
   "metadata": {},
   "source": [
    "Sector Wise Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6894c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sector-wise stock data\n",
    "for sector, stocks in preprocessed_data.items():\n",
    "    print(f\"\\nVisualizing data for sector: {sector}\\n\")\n",
    "    \n",
    "    # Combine all stocks in the sector into one DataFrame for visualization\n",
    "    combined_df = pd.DataFrame()\n",
    "    for stock, df in stocks.items():\n",
    "        df_temp = df.copy()\n",
    "        df_temp['stock'] = stock  # Add a column to identify the stock\n",
    "        combined_df = pd.concat([combined_df, df_temp], ignore_index=True)\n",
    "\n",
    "    # Plot closing prices for all stocks in the sector\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sns.lineplot(data=combined_df, x='date', y='close', hue='stock')\n",
    "    plt.title(f\"Closing Prices for {sector.capitalize()} Sector\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Closing Price\")\n",
    "    plt.legend(title=\"Stock\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot daily returns distribution\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    for stock, df in stocks.items():\n",
    "        sns.kdeplot(df['Daily Return'].dropna(), label=stock)\n",
    "    plt.title(f\"Daily Returns Distribution for {sector.capitalize()} Sector\")\n",
    "    plt.xlabel(\"Daily Return\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend(title=\"Stock\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot EMA comparison for a representative stock\n",
    "    sample_stock = list(stocks.keys())[0]\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(stocks[sample_stock]['date'], stocks[sample_stock]['close'], label='Close Price')\n",
    "    plt.plot(stocks[sample_stock]['date'], stocks[sample_stock]['ema7'], label='EMA 7 days')\n",
    "    plt.plot(stocks[sample_stock]['date'], stocks[sample_stock]['ema30'], label='EMA 30 days')\n",
    "    plt.plot(stocks[sample_stock]['date'], stocks[sample_stock]['ema60'], label='EMA 60 days')\n",
    "    plt.title(f\"EMA Comparison for {sample_stock} in {sector.capitalize()} Sector\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86781c8",
   "metadata": {},
   "source": [
    "BiLSTM Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dde898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "def split_data(df, test_size=0.2):\n",
    "    train_size = int(len(df) * (1 - test_size))\n",
    "    train_data = df[:train_size]\n",
    "    test_data = df[train_size:]\n",
    "    return train_data, test_data\n",
    "\n",
    "# Prepare data for LSTM model\n",
    "def prepare_data_for_lstm(df, target_col='close', lookback=60):\n",
    "    \"\"\"Prepare data for LSTM model with a lookback period\"\"\"\n",
    "    # Scale the data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = scaler.fit_transform(df[target_col].values.reshape(-1, 1))\n",
    "    \n",
    "    X, y = [], []\n",
    "    for i in range(lookback, len(scaled_data)):\n",
    "        X.append(scaled_data[i-lookback:i, 0])\n",
    "        y.append(scaled_data[i, 0])\n",
    "    \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "    \n",
    "    return X, y, scaler\n",
    "\n",
    "# Create Bidirectional LSTM model for improved performance\n",
    "def create_bilstm_model(input_shape):\n",
    "    \"\"\"Create Bidirectional LSTM model for stock price prediction\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units=50, return_sequences=True, input_shape=input_shape)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Bidirectional(LSTM(units=50, return_sequences=False)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=1))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the model\n",
    "def train_lstm_model(df, lookback=60, epochs=12, batch_size=32, test_size=0.2):\n",
    "    \"\"\"Train and evaluate LSTM model\"\"\"\n",
    "    # Split data\n",
    "    train_data, test_data = split_data(df, test_size)\n",
    "    \n",
    "    # Prepare data for LSTM\n",
    "    X_train, y_train, scaler = prepare_data_for_lstm(train_data, lookback=lookback)\n",
    "    X_test, y_test, _ = prepare_data_for_lstm(pd.concat([train_data.iloc[-lookback:], test_data]), lookback=lookback)\n",
    "    \n",
    "    # Create and train model\n",
    "    model = create_bilstm_model((X_train.shape[1], 1))\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, \n",
    "                        validation_data=(X_test, y_test), verbose=1)\n",
    "    \n",
    "    # Predictions\n",
    "    train_predictions = model.predict(X_train)\n",
    "    test_predictions = model.predict(X_test)\n",
    "    \n",
    "    # Inverse scaling\n",
    "    train_predictions = scaler.inverse_transform(train_predictions)\n",
    "    y_train_actual = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "    test_predictions = scaler.inverse_transform(test_predictions)\n",
    "    y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "    return model, scaler, test_predictions, y_test_actual, test_data['date'].reset_index(drop=True), history\n",
    "\n",
    "# EMA constraint function\n",
    "def apply_ema_constraint(prediction, df, window=60, max_deviation=0.1):\n",
    "    \"\"\"Apply EMA constraint to ensure prediction is within reasonable range\"\"\"\n",
    "    # Calculate EMA of actual prices\n",
    "    ema = df['close'].ewm(span=window, adjust=False).mean().iloc[-1]\n",
    "    \n",
    "    # Set upper and lower bounds based on EMA\n",
    "    upper_bound = ema * (1 + max_deviation)\n",
    "    lower_bound = ema * (1 - max_deviation)\n",
    "    \n",
    "    # Apply constraints\n",
    "    if prediction > upper_bound:\n",
    "        constrained_prediction = upper_bound\n",
    "    elif prediction < lower_bound:\n",
    "        constrained_prediction = lower_bound\n",
    "    else:\n",
    "        constrained_prediction = prediction\n",
    "    \n",
    "    return constrained_prediction, ema, lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2065ab",
   "metadata": {},
   "source": [
    "Training and Saving Models for each Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a7730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Train and save LSTM models for each stock\n",
    "trained_models = {}\n",
    "\n",
    "for sector, stocks in preprocessed_data.items():\n",
    "    trained_models[sector] = {}\n",
    "    for stock, df in stocks.items():\n",
    "        print(f\"\\nTraining LSTM model for {stock} in {sector} sector...\")\n",
    "        \n",
    "        # Prepare data\n",
    "        lookback = 60\n",
    "        if len(df) <= lookback:\n",
    "            print(f\"Not enough data for {stock}. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Train model\n",
    "        model, scaler, test_predictions, y_test_actual, test_dates, history = train_lstm_model(\n",
    "            df, lookback=lookback, epochs=12, batch_size=32\n",
    "        )\n",
    "        \n",
    "        # Plot actual vs predicted prices for test set\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.plot(test_dates, y_test_actual, label='Actual Prices')\n",
    "        plt.plot(test_dates, test_predictions, label='Predicted Prices')\n",
    "        plt.title(f'Stock Price Prediction for {stock}')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Price')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        # Save model and scaler\n",
    "        model_name = f\"{sector}_{stock}_lstm_model.keras\"\n",
    "        scaler_name = f\"{sector}_{stock}_scaler.joblib\"\n",
    "        model.save(model_name)\n",
    "        joblib.dump(scaler, scaler_name)\n",
    "        \n",
    "        trained_models[sector][stock] = {\n",
    "            \"model_file\": model_name,\n",
    "            \"scaler_file\": scaler_name\n",
    "        }\n",
    "        \n",
    "        print(f\"Model and scaler saved for {stock}.\")\n",
    "\n",
    "# Output the trained models dictionary\n",
    "trained_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7095859b",
   "metadata": {},
   "source": [
    "Next Day Price Prediction for All Stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9a2b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a saved model and scaler\n",
    "def load_model_and_scaler(model_file, scaler_file):\n",
    "    model = load_model(model_file)\n",
    "    scaler = joblib.load(scaler_file)\n",
    "    return model, scaler\n",
    "\n",
    "# Function to predict next day's price\n",
    "def predict_next_day(model, df, scaler, lookback=60, ema_window=60, max_deviation=0.1):\n",
    "    \"\"\"Predict next day's stock price and apply EMA constraint\"\"\"\n",
    "    # Get the last 'lookback' days of data\n",
    "    last_sequence = df['close'].values[-lookback:]\n",
    "    \n",
    "    # Scale the data\n",
    "    last_sequence = scaler.transform(last_sequence.reshape(-1, 1))\n",
    "    \n",
    "    # Reshape for LSTM input\n",
    "    last_sequence = np.reshape(last_sequence, (1, lookback, 1))\n",
    "    \n",
    "    # Predict\n",
    "    next_day_scaled = model.predict(last_sequence)\n",
    "    \n",
    "    # Inverse transform\n",
    "    next_day_price = scaler.inverse_transform(next_day_scaled)[0, 0]\n",
    "    \n",
    "    # Apply EMA constraint\n",
    "    constrained_price, ema, lower_bound, upper_bound = apply_ema_constraint(\n",
    "        next_day_price, df, window=ema_window, max_deviation=max_deviation\n",
    "    )\n",
    "    \n",
    "    return constrained_price, next_day_price, ema, lower_bound, upper_bound\n",
    "\n",
    "# Predict next day prices for all stocks\n",
    "predictions = {}\n",
    "\n",
    "for sector, stocks in trained_models.items():\n",
    "    predictions[sector] = {}\n",
    "    print(f\"\\n=== Predictions for {sector.upper()} sector ===\\n\")\n",
    "    \n",
    "    for stock, model_info in stocks.items():\n",
    "        # Load model and scaler\n",
    "        model, scaler = load_model_and_scaler(model_info[\"model_file\"], model_info[\"scaler_file\"])\n",
    "        \n",
    "        # Get the stock data\n",
    "        df = preprocessed_data[sector][stock]\n",
    "        \n",
    "        # Predict next day price\n",
    "        constrained_price, raw_prediction, ema, lower_bound, upper_bound = predict_next_day(\n",
    "            model, df, scaler, lookback=60, ema_window=60, max_deviation=0.1\n",
    "        )\n",
    "        \n",
    "        # Store prediction\n",
    "        predictions[sector][stock] = {\n",
    "            \"current_price\": df['close'].iloc[-1],\n",
    "            \"raw_prediction\": raw_prediction,\n",
    "            \"constrained_prediction\": constrained_price,\n",
    "            \"ema_60\": ema,\n",
    "            \"lower_bound\": lower_bound,\n",
    "            \"upper_bound\": upper_bound\n",
    "        }\n",
    "        \n",
    "        # Print prediction\n",
    "        print(f\"{stock}:\")\n",
    "        print(f\"  Current price: ${df['close'].iloc[-1]:.2f}\")\n",
    "        print(f\"  Raw prediction: ${raw_prediction:.2f}\")\n",
    "        print(f\"  EMA (60 days): ${ema:.2f}\")\n",
    "        print(f\"  Allowed range: ${lower_bound:.2f} to ${upper_bound:.2f}\")\n",
    "        print(f\"  Final prediction: ${constrained_price:.2f}\")\n",
    "        print(f\"  Expected change: ${constrained_price - df['close'].iloc[-1]:.2f} ({(constrained_price - df['close'].iloc[-1]) / df['close'].iloc[-1] * 100:.2f}%)\")\n",
    "        print()\n",
    "\n",
    "# Create a summary DataFrame of all predictions\n",
    "prediction_summary = pd.DataFrame(columns=[\n",
    "    'Sector', 'Stock', 'Current Price', 'Predicted Price', 'Expected Change', 'Expected Change (%)'\n",
    "])\n",
    "\n",
    "row = 0\n",
    "for sector, stocks in predictions.items():\n",
    "    for stock, pred in stocks.items():\n",
    "        prediction_summary.loc[row] = [\n",
    "            sector,\n",
    "            stock,\n",
    "            pred['current_price'],\n",
    "            pred['constrained_prediction'],\n",
    "            pred['constrained_prediction'] - pred['current_price'],\n",
    "            (pred['constrained_prediction'] - pred['current_price']) / pred['current_price'] * 100\n",
    "        ]\n",
    "        row += 1\n",
    "\n",
    "# Sort by expected change percentage\n",
    "prediction_summary = prediction_summary.sort_values('Expected Change (%)', ascending=False)\n",
    "prediction_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970daf2f",
   "metadata": {},
   "source": [
    "Visualizing Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28431548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of predicted changes across all stocks\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.barplot(data=prediction_summary, x='Stock', y='Expected Change (%)', hue='Sector')\n",
    "plt.title('Predicted Price Changes for Next Trading Day')\n",
    "plt.xlabel('Stock')\n",
    "plt.ylabel('Expected Change (%)')\n",
    "plt.xticks(rotation=90)\n",
    "plt.axhline(y=0, color='r', linestyle='-')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create visualization of sector-wise average predictions\n",
    "sector_avg = prediction_summary.groupby('Sector')['Expected Change (%)'].mean().reset_index()\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=sector_avg, x='Sector', y='Expected Change (%)')\n",
    "plt.title('Average Predicted Change by Sector')\n",
    "plt.xlabel('Sector')\n",
    "plt.ylabel('Average Expected Change (%)')\n",
    "plt.axhline(y=0, color='r', linestyle='-')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a price range visualization for top 10 stocks\n",
    "top_10 = prediction_summary.head(10)\n",
    "plt.figure(figsize=(14, 7))\n",
    "for i, row in top_10.iterrows():\n",
    "    sector = row['Sector']\n",
    "    stock = row['Stock']\n",
    "    current = row['Current Price']\n",
    "    predicted = row['Predicted Price']\n",
    "    pred_info = predictions[sector][stock]\n",
    "    \n",
    "    plt.scatter(stock, current, color='blue', s=100, label='Current Price' if i==0 else \"\")\n",
    "    plt.scatter(stock, predicted, color='green', s=100, label='Predicted Price' if i==0 else \"\")\n",
    "    plt.plot([stock, stock], [pred_info['lower_bound'], pred_info['upper_bound']], \n",
    "             color='red', linestyle='-', label='Allowed Range' if i==0 else \"\")\n",
    "    \n",
    "plt.title('Price Prediction with EMA Constraints for Top 10 Stocks')\n",
    "plt.xlabel('Stock')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4284914a",
   "metadata": {},
   "source": [
    "NOVELTY: Bayesian Networks to evaluate External Market Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f226063b",
   "metadata": {},
   "source": [
    "Front Matter 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826a7481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from fredapi import Fred\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725560f2",
   "metadata": {},
   "source": [
    "Data Collection and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46196b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FRED API (use your API key)\n",
    "fred = Fred(api_key='YOUR_FRED_API_KEY')\n",
    "\n",
    "# Define date range\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2023-12-31'\n",
    "\n",
    "# IBM stock data\n",
    "ibm = yf.download('IBM', start=start_date, end=end_date)  # IBM stock price & volume\n",
    "print(\"IBM Columns:\", ibm.columns)\n",
    "\n",
    "# IBM Option Chain for Implied Volatility\n",
    "ticker = yf.Ticker('IBM')\n",
    "options_dates = ticker.options\n",
    "option_chain = ticker.option_chain(options_dates[0])  # Take nearest expiry\n",
    "ibm_iv = option_chain.calls['impliedVolatility'].mean()  # Average IV from calls\n",
    "\n",
    "# Market index (S&P 500)\n",
    "sp500 = yf.download('^GSPC', start=start_date, end=end_date)  # Market Index Return\n",
    "\n",
    "# Sector ETF (Technology ETF XLK)\n",
    "xlk = yf.download('XLK', start=start_date, end=end_date)  # Sector Performance Index\n",
    "\n",
    "# VIX index (daily volatility measure)\n",
    "vix = yf.download('^VIX', start=start_date, end=end_date)  # VIX (CBOE Volatility Index)\n",
    "\n",
    "# Credit Spread proxy using BAA - AAA spread from FRED\n",
    "baa_yield = fred.get_series('BAA10Y', observation_start=start_date, observation_end=end_date)  # BAA corporate bonds\n",
    "aaa_yield = fred.get_series('AAA10Y', observation_start=start_date, observation_end=end_date)  # AAA corporate bonds\n",
    "credit_spread = (baa_yield - aaa_yield).resample('D').ffill()  # Credit Spreads\n",
    "\n",
    "# 10-Year Treasury Yield from FRED\n",
    "treasury_yield = fred.get_series('GS10', observation_start=start_date, observation_end=end_date)  # Risk-free rate\n",
    "treasury_yield = treasury_yield.resample('D').ffill()\n",
    "\n",
    "# CPI data from FRED\n",
    "cpi = fred.get_series('CPIAUCSL', observation_start=start_date, observation_end=end_date)  # Inflation proxy\n",
    "cpi = cpi.resample('D').ffill()\n",
    "\n",
    "# Align all data to the same date index\n",
    "combined_data = pd.DataFrame(index=ibm.index)\n",
    "combined_data['IBM_Close'] = ibm['Close']  # IBM Close\n",
    "combined_data['IBM_Volume'] = ibm['Volume']  # Trading Volume\n",
    "combined_data['SP500_Close'] = sp500['Close']  # Market Index Return\n",
    "combined_data['XLK_Close'] = xlk['Close']  # Sector Performance Index\n",
    "combined_data['VIX'] = vix['Close']  # VIX\n",
    "combined_data['Treasury_Yield'] = treasury_yield.reindex(ibm.index)  # Treasury Yield\n",
    "combined_data['CPI'] = cpi.reindex(ibm.index)  # CPI\n",
    "combined_data['Credit_Spread'] = credit_spread.reindex(ibm.index)  # Credit Spread\n",
    "combined_data['Implied_Volatility'] = ibm_iv  # Static IV for now\n",
    "\n",
    "# Drop rows with missing values\n",
    "combined_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4315c95",
   "metadata": {},
   "source": [
    "Correlation Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86242595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IBM daily returns\n",
    "combined_data['IBM_Returns'] = combined_data['IBM_Close'].pct_change()\n",
    "\n",
    "# Calculate company-specific features directly from combined_data\n",
    "combined_data['IBM_Volatility'] = combined_data['IBM_Close'].pct_change().rolling(window=30).std()  # Historical Volatility\n",
    "combined_data['IBM_Momentum'] = combined_data['IBM_Close'] / combined_data['IBM_Close'].shift(20) - 1  # Price Momentum (20-day)\n",
    "\n",
    "# Calculate daily returns for other relevant factors\n",
    "combined_data['SP500_Returns'] = combined_data['SP500_Close'].pct_change()\n",
    "combined_data['XLK_Returns'] = combined_data['XLK_Close'].pct_change()\n",
    "combined_data['VIX_Change'] = combined_data['VIX'].pct_change()\n",
    "combined_data['Treasury_Yield_Change'] = combined_data['Treasury_Yield'].pct_change()\n",
    "combined_data['CPI_Change'] = combined_data['CPI'].pct_change()\n",
    "combined_data['Credit_Spread_Change'] = combined_data['Credit_Spread'].pct_change()\n",
    "combined_data['Volume_Change'] = combined_data['IBM_Volume'].pct_change()\n",
    "\n",
    "# Drop NA after pct_change\n",
    "combined_data.dropna(inplace=True)\n",
    "\n",
    "# Calculate threshold values for each factor based on percentiles\n",
    "thresholds = {}\n",
    "for factor in [\n",
    "    'SP500_Returns', 'XLK_Returns', 'VIX_Change', 'Treasury_Yield_Change',\n",
    "    'CPI_Change', 'Credit_Spread_Change', 'IBM_Volatility', 'IBM_Momentum',\n",
    "    'Volume_Change', 'Implied_Volatility'\n",
    "]:\n",
    "    if factor == 'Implied_Volatility':\n",
    "        thresholds[factor] = [combined_data[factor].mean() * 0.85, combined_data[factor].mean() * 1.15]\n",
    "    else:\n",
    "        thresholds[factor] = [\n",
    "            combined_data[factor].quantile(0.33),\n",
    "            combined_data[factor].quantile(0.66)\n",
    "        ]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = combined_data[[\n",
    "    'IBM_Returns', 'SP500_Returns', 'XLK_Returns', 'VIX_Change', \n",
    "    'Treasury_Yield_Change', 'CPI_Change', 'Credit_Spread_Change',\n",
    "    'IBM_Volatility', 'IBM_Momentum', 'Volume_Change', 'Implied_Volatility'\n",
    "]].corr()\n",
    "\n",
    "# Show correlations specifically to IBM Returns\n",
    "ibm_corr = correlation_matrix['IBM_Returns'].drop('IBM_Returns')\n",
    "\n",
    "print(\"Correlation of external factors with IBM Returns:\")\n",
    "print(ibm_corr)\n",
    "\n",
    "# Optional: Plot heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0b2cef",
   "metadata": {},
   "source": [
    "Construct Conditional Probability Table for External Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367bb94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cpt_value(value, thresholds, correlation):\n",
    "    shift = abs(correlation) * 0.25\n",
    "    base = 1 / 3\n",
    "    if value < thresholds[0]:\n",
    "        return base - shift if correlation > 0 else base + shift\n",
    "    elif value > thresholds[1]:\n",
    "        return base + shift if correlation > 0 else base - shift\n",
    "    else:\n",
    "        return base\n",
    "\n",
    "# Use most recent values for CPT evaluation\n",
    "latest = combined_data.iloc[-1]\n",
    "\n",
    "# Compute conditional probabilities\n",
    "cp_values = {}\n",
    "for factor, corr in ibm_corr.items():\n",
    "    val = latest[factor]\n",
    "    t = thresholds[factor]\n",
    "    cp = get_cpt_value(val, t, corr)\n",
    "    cp_values[factor] = cp\n",
    "    print(f\"{factor}: value={val:.4f}, correlation={corr:.3f}, CP={cp:.3f}\")\n",
    "\n",
    "# Compute overall conditional probability (product of independent CPs)\n",
    "risk_factor = 1\n",
    "for cp in cp_values.values():\n",
    "    risk_factor *= cp\n",
    "\n",
    "print(\"\\nFinal Conditional Risk Factor for IBM:\", risk_factor)\n",
    "\n",
    "# Example\n",
    "predicted_price = predictions['technology']['IBM']['constrained_prediction']\n",
    "adjusted_price = predicted_price * risk_factor\n",
    "print(\"Predicted Price:\", predicted_price)\n",
    "print(\"Adjusted Price after Risk Assessment:\", adjusted_price)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
